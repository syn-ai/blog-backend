{
  "main.py": "from fastapi import FastAPI\nfrom src.blog_posts.hackmd import router as blog_router\n\napp = FastAPI()\napp.include_router(blog_router)\n\nif __name__ == \"__main__\":\n    app.run(\"main:app\", host=\"0.0.0.0\", port=4050, reload=True)",
  "src/main.py": "import uvicorn\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom src.utils.static_manager import StaticManager\nfrom src.blog_posts.hackmd import router as blog_router\n\n\napp = FastAPI()\nstatic_manager = StaticManager()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],    \n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/api/backups\")\nasync def list_backups(filename: str | None = None):\n    \"\"\"List all backups, optionally filtered by original filename.\"\"\"\n    return static_manager.list_backups(filename)\n\n# Include your blog router\napp.include_router(blog_router)\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=4050, reload=True)",
  "src/__init__.py": "",
  "src/utils/walkdir.py": "import os\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict\n\nclass CodeCollector:\n    \"\"\"\n    Utility for walking directories and collecting Python source code files.\n    \n    Attributes:\n        excluded_paths: List of path patterns to exclude\n        output_file: Path to the output JSON file\n    \"\"\"\n    \n    def __init__(self):\n        self.excluded_paths = [\n            'node_modules', 'venv', '.env', '.git',\n            'build', 'dist', '__pycache__', \n            '.pytest_cache', '.vscode', 'site-packages'\n        ]\n        self.output_file = 'code_collection.json'\n\n    def is_path_excluded(self, path: str) -> bool:\n        \"\"\"\n        Check if path should be excluded based on patterns.\n        \n        Args:\n            path: Path to check\n            \n        Returns:\n            bool: True if path should be excluded\n        \"\"\"\n        return any(excluded in path for excluded in self.excluded_paths)\n\n    def collect_python_files(self, root_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Walk through directory and collect Python files.\n        \n        Args:\n            root_dir: Root directory to start walking from\n            \n        Returns:\n            Dict[str, str]: Dictionary mapping file paths to their content\n        \"\"\"\n        collected_files = {}\n        \n        for dirpath, dirnames, filenames in os.walk(root_dir):\n            # Skip excluded directories\n            dirnames[:] = [d for d in dirnames if not self.is_path_excluded(d)]\n            \n            for filename in filenames:\n                if filename.endswith('.py'):\n                    full_path = os.path.join(dirpath, filename)\n                    \n                    # Skip if path contains excluded patterns\n                    if self.is_path_excluded(full_path):\n                        continue\n                    \n                    try:\n                        with open(full_path, 'r', encoding='utf-8') as f:\n                            content = f.read()\n                            # Store relative path as key\n                            rel_path = os.path.relpath(full_path, root_dir)\n                            collected_files[rel_path] = content\n                    except Exception as e:\n                        print(f\"Error reading {full_path}: {e}\")\n                        \n        return collected_files\n\n    def save_collection(self, collection: Dict[str, str]) -> None:\n        \"\"\"\n        Save collected files to JSON.\n        \n        Args:\n            collection: Dictionary of collected files\n        \"\"\"\n        try:\n            with open(self.output_file, 'w', encoding='utf-8') as f:\n                json.dump(collection, f, indent=2)\n        except Exception as e:\n            print(f\"Error saving collection: {e}\")\n\n    def load_collection(self) -> Dict[str, str]:\n        \"\"\"\n        Load previously collected files from JSON.\n        \n        Returns:\n            Dict[str, str]: Dictionary of collected files\n        \"\"\"\n        try:\n            with open(self.output_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception as e:\n            print(f\"Error loading collection: {e}\")\n            return {}\n\n    def collect_and_save(self, root_dir: str) -> Dict[str, str]:\n        \"\"\"\n        Collect Python files and save to JSON in one operation.\n        \n        Args:\n            root_dir: Root directory to start walking from\n            \n        Returns:\n            Dict[str, str]: Dictionary of collected files\n        \"\"\"\n        collection = self.collect_python_files(root_dir)\n        self.save_collection(collection)\n        return collection\n\ndef main():\n    \"\"\"Main function to demonstrate usage.\"\"\"\n    collector = CodeCollector()\n    \n    # Use current directory as root\n    root_dir = '.'\n    \n    print(\"Collecting Python files...\")\n    collection = collector.collect_and_save(root_dir)\n    \n    print(\"\\nCollected files:\")\n    for path in collection.keys():\n        print(path) \n        \n        \nif __name__ == '__main__':\n    main()",
  "src/utils/static_manager.py": "import os\nimport json\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\nfrom datetime import datetime\nfrom fastapi import HTTPException\n\nclass StaticManager:\n    \"\"\"\n    Utility class for managing static files and directories.\n    \n    Attributes:\n        base_dir: Base directory for static files\n        backup_dir: Directory for backups\n    \"\"\"\n    def __init__(self, base_dir: str = \"data\"):\n        self.base_dir = Path(base_dir)\n        self.backup_dir = self.base_dir / \"backups\"\n        self._ensure_directories()\n\n    def _ensure_directories(self) -> None:\n        \"\"\"Create necessary directories if they don't exist.\"\"\"\n        self.base_dir.mkdir(exist_ok=True)\n        self.backup_dir.mkdir(exist_ok=True)\n\n    def save_json(self, filename: str, data: Any) -> Path:\n        \"\"\"\n        Save data as JSON file in the static directory.\n        \n        Args:\n            filename: Name of the file (with or without .json extension)\n            data: Data to save (must be JSON serializable)\n            \n        Returns:\n            Path: Path to the saved file\n            \n        Raises:\n            HTTPException: If file operation fails\n        \"\"\"\n        if not filename.endswith('.json'):\n            filename = f\"{filename}.json\"\n        \n        file_path = self.base_dir / filename\n        try:\n            with file_path.open('w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2, ensure_ascii=False)\n            return file_path\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Failed to save file: {str(e)}\")\n\n    def load_json(self, filename: str) -> Any:\n        \"\"\"\n        Load data from a JSON file.\n        \n        Args:\n            filename: Name of the file (with or without .json extension)\n            \n        Returns:\n            Any: Parsed JSON data\n            \n        Raises:\n            HTTPException: If file doesn't exist or is invalid\n        \"\"\"\n        if not filename.endswith('.json'):\n            filename = f\"{filename}.json\"\n            \n        file_path = self.base_dir / filename\n        try:\n            with file_path.open('r', encoding='utf-8') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise HTTPException(status_code=404, detail=f\"File {filename} not found\")\n        except json.JSONDecodeError:\n            raise HTTPException(status_code=500, detail=f\"Invalid JSON in {filename}\")\n\n    def create_backup(self, filename: str) -> Path:\n        \"\"\"\n        Create a backup of a file with timestamp.\n        \n        Args:\n            filename: Name of the file to backup\n            \n        Returns:\n            Path: Path to the backup file\n            \n        Raises:\n            HTTPException: If backup operation fails\n        \"\"\"\n        source_path = self.base_dir / filename\n        if not source_path.exists():\n            raise HTTPException(status_code=404, detail=f\"File {filename} not found\")\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        backup_filename = f\"{filename.rsplit('.', 1)[0]}_{timestamp}.{filename.rsplit('.', 1)[1]}\"\n        backup_path = self.backup_dir / backup_filename\n\n        try:\n            shutil.copy2(source_path, backup_path)\n            return backup_path\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Backup failed: {str(e)}\")\n\n    def list_files(self, extension: Optional[str] = None) -> List[str]:\n        \"\"\"\n        List all files in the static directory.\n        \n        Args:\n            extension: Optional file extension filter (e.g., 'json')\n            \n        Returns:\n            List[str]: List of filenames\n        \"\"\"\n        files = []\n        for file in self.base_dir.iterdir():\n            if file.is_file():\n                if extension:\n                    if file.suffix == f\".{extension}\":\n                        files.append(file.name)\n                else:\n                    files.append(file.name)\n        return files\n\n    def list_backups(self, original_filename: Optional[str] = None) -> List[str]:\n        \"\"\"\n        List all backups, optionally filtered by original filename.\n        \n        Args:\n            original_filename: Optional original filename to filter backups\n            \n        Returns:\n            List[str]: List of backup filenames\n        \"\"\"\n        backups = []\n        for file in self.backup_dir.iterdir():\n            if file.is_file():\n                if original_filename:\n                    if file.name.startswith(original_filename.rsplit('.', 1)[0]):\n                        backups.append(file.name)\n                else:\n                    backups.append(file.name)\n        return backups\n\n    def delete_file(self, filename: str, create_backup: bool = True) -> None:\n        \"\"\"\n        Delete a file from the static directory.\n        \n        Args:\n            filename: Name of the file to delete\n            create_backup: Whether to create a backup before deletion\n            \n        Raises:\n            HTTPException: If deletion fails\n        \"\"\"\n        file_path = self.base_dir / filename\n        if not file_path.exists():\n            raise HTTPException(status_code=404, detail=f\"File {filename} not found\")\n\n        try:\n            if create_backup:\n                self.create_backup(filename)\n            file_path.unlink()\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Deletion failed: {str(e)}\")\n\n    def restore_backup(self, backup_filename: str) -> Path:\n        \"\"\"\n        Restore a file from backup.\n        \n        Args:\n            backup_filename: Name of the backup file to restore\n            \n        Returns:\n            Path: Path to the restored file\n            \n        Raises:\n            HTTPException: If restoration fails\n        \"\"\"\n        backup_path = self.backup_dir / backup_filename\n        if not backup_path.exists():\n            raise HTTPException(status_code=404, detail=f\"Backup {backup_filename} not found\")\n\n        original_filename = backup_filename.split('_')[0] + '.' + backup_filename.rsplit('.', 1)[1]\n        target_path = self.base_dir / original_filename\n\n        try:\n            if target_path.exists():\n                self.create_backup(original_filename)\n            shutil.copy2(backup_path, target_path)\n            return target_path\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Restore failed: {str(e)}\") ",
  "src/utils/__init__.py": "",
  "src/blog_posts/hackmd.py": "import os\nimport json\nfrom typing import List\nimport requests\nfrom pathlib import Path\nfrom pydantic import BaseModel\nfrom dotenv import load_dotenv\nfrom fastapi import HTTPException, APIRouter\n\nload_dotenv()\n\nHACKMD_API_URL = 'https://api.hackmd.io/v1'\nrouter = APIRouter()\n\nclass BlogPost(BaseModel):\n    \"\"\"\n    Pydantic model representing a blog post.\n    \n    Attributes:\n        id: Unique identifier for the post\n        title: Title of the blog post\n        content: Full content of the blog post\n        publishDate: Date when the post was published\n        lastModified: Date when the post was last modified\n        excerpt: Short summary of the post content\n        slug: URL-friendly identifier for the post\n        coverImage: Optional URL to the post's cover image\n        readingTime: Optional estimated reading time\n    \"\"\"\n    id: str\n    title: str\n    content: str\n    publishDate: str\n    lastModified: str\n    excerpt: str\n    slug: str\n    coverImage: str | None = None\n    readingTime: str | None = None\n\ndef get_from_cache() -> List[BlogPost] | None:\n    \"\"\"\n    Retrieve blog posts from the local cache file.\n    \n    Returns:\n        List[BlogPost]: List of cached blog posts if cache exists\n        None: If cache doesn't exist or is invalid\n    \"\"\"\n    cache_path = Path(\"data/posts_cache.json\")\n    if cache_path.exists():\n        return [BlogPost(**post) for post in json.loads(cache_path.read_text())]\n    return None\n\ndef save_to_cache(posts: List[BlogPost]) -> None:\n    \"\"\"\n    Save blog posts to the local cache file.\n    \n    Args:\n        posts: List of BlogPost objects to cache\n    \"\"\"\n    cache_path = Path(\"data/posts_cache.json\")\n    cache_path.parent.mkdir(parents=True, exist_ok=True)\n    cache_path.write_text(json.dumps([post.dict() for post in posts]))\n\n@router.get(\"/posts\", response_model=List[BlogPost])\nasync def fetch_blog_posts():\n    \"\"\"\n    Fetch all blog posts from cache or HackMD API.\n    \n    Returns:\n        List[BlogPost]: List of all blog posts\n        \n    Raises:\n        HTTPException: If API request fails or returns invalid data\n    \"\"\"\n    if cached_posts := get_from_cache():\n        return cached_posts\n\n    # If not in cache, fetch from API\n    headers = {\"Authorization\": f\"Bearer {os.getenv('HACKMD_API_KEY')}\"}\n\n    try:\n        response = requests.get(f\"{HACKMD_API_URL}/notes\", headers=headers)\n        response.raise_for_status()\n        posts = response.json()\n    except requests.exceptions.RequestException as err:\n        raise HTTPException(status_code=500, detail=f\"Failed to fetch blog posts: {err}\")\n\n    # Transform to our BlogPost model\n    transformed_posts = [\n        BlogPost(\n            id=post[\"id\"],\n            title=post[\"title\"],\n            content=post[\"content\"],\n            publishDate=post[\"publishedAt\"],\n            lastModified=post[\"lastChangedAt\"],\n            excerpt=post.get(\"excerpt\") or post[\"content\"][:150] + \"...\",\n            slug=post[\"permalink\"]\n        ) for post in posts\n    ]\n\n    # Save to cache\n    save_to_cache(transformed_posts)\n    return transformed_posts\n\n@router.get(\"/posts/{slug}\", response_model=BlogPost)\nasync def fetch_blog_post(slug: str):\n    \"\"\"\n    Fetch a single blog post by its slug.\n    \n    Args:\n        slug: URL-friendly identifier for the post\n        \n    Returns:\n        BlogPost: Single blog post matching the slug\n        \n    Raises:\n        HTTPException: If post not found or API request fails\n    \"\"\"\n    if cached_posts := get_from_cache():\n        for post in cached_posts:\n            if post.slug == slug:\n                return post\n\n    # If not found in cache, fetch from API\n    headers = {\"Authorization\": f\"Bearer {os.getenv('HACKMD_API_KEY')}\"}\n\n    try:\n        response = requests.get(f\"{HACKMD_API_URL}/notes/{slug}\", headers=headers)\n        response.raise_for_status()\n        post = response.json()\n    except requests.exceptions.RequestException as err:\n        raise HTTPException(status_code=500, detail=f\"Failed to fetch blog post: {err}\")\n\n    return BlogPost(\n        id=post[\"id\"],\n        title=post[\"title\"],\n        content=post[\"content\"],\n        publishDate=post[\"publishedAt\"],\n        lastModified=post[\"lastChangedAt\"],\n        excerpt=post.get(\"excerpt\") or post[\"content\"][:150] + \"...\",\n        slug=post[\"permalink\"]\n    )\n\n@router.post(\"/posts/refresh\", response_model=List[BlogPost])\nasync def refresh_blog_posts():\n    \"\"\"\n    Force refresh of blog posts cache.\n    \n    Returns:\n        List[BlogPost]: Updated list of all blog posts\n        \n    Raises:\n        HTTPException: If refresh operation fails\n    \"\"\"\n    posts = await fetch_blog_posts()\n    save_to_cache(posts)\n    return posts ",
  "src/blog_posts/__init__.py": ""
}